---
title: "Model Tuning"
subtitle: "useR2022"
author: "Emil Hvitfelt"
format: 
  revealjs:
    theme: [default, styles.scss]
    width: 1600
    height: 900
knitr:
  opts_chunk: 
    echo: true
    collapse: true
    comment: "#>"
---

```{r previously, include = FALSE}
library(tidymodels)
library(elevators)
number_extractor <- function(x) {
  x <- stringr::str_extract(x, "[0-9]+")
  x <- as.integer(x)
  x[x > 100] <- NA
  x
}

elevators_cleaned <- elevators %>%
  mutate(speed_fpm = log(speed_fpm + 0.5),
         floor_from = number_extractor(floor_from),
         floor_to = number_extractor(floor_to),
         travel_distance = number_extractor(travel_distance)) %>%
  select(-device_number, -bin, -tax_block, -tax_lot, -house_number, 
         -street_name, -zip_code)

set.seed(1234)
elevators_split <- initial_split(elevators_cleaned)
elevators_split

elevators_train <- training(elevators_split)
elevators_test  <- testing(elevators_split)

set.seed(2453)
elevators_folds <- vfold_cv(elevators_train)

elevators_rec <- 
  recipe(speed_fpm ~ ., data = elevators_train) %>% 
  step_date(approval_date, lastper_insp_date, keep_original_cols = FALSE) %>%
  step_impute_mean(all_numeric_predictors()) %>%
  step_novel(all_nominal_predictors()) %>%
  step_unknown(all_nominal_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>% 
  step_zv(all_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>% 
  step_corr(all_numeric_predictors(), threshold = 0.9)

lm_spec <- 
  linear_reg(penalty = tune(), mixture = tune()) %>% 
  set_engine("glmnet")

elevators_wflow <- 
  workflow() %>% 
  add_model(lm_spec) %>% 
  add_recipe(elevators_rec)
```


# Tuning parameters

These are model or preprocessing parameters that are important but cannot be estimated directly form the data. 

Some examples:

::: columns
::: {.column width="50%"}
* Tree depth in decision trees.
* Number of neighbors in a K-nearest neighbor model. 
* Activation function (e.g. sigmoidal, ReLu) in neural networks. 
* Number of PCA components to retain
:::

::: {.column width="50%"}
* Covariance/correlation matrix structure in mixed models.
* Data distribution in survival models.
* Spline degrees of freedom. 
:::
:::

# Optimizing tuning parameters

The main approach is to try different values and measure their performance. This can lead us to good values for these parameters. 

The main two classes of optimization models are: 

 * _Grid search_ where a pre-defined set of candidate values are tested. 
 * _Iterative search_ methods suggest/estimate new values of candidate parameters to evaluate. 

Once the value(s) of the parameter(s) are determine, a model can be finalized but fitting the model to the entire training set. 

# Measuring tuning paramters

We need performance metrics to tell us which candidate values are good and which are not. 

Using the test set, or simply re-predicting the training set, are very bad ideas. 

Since tuning parameters often control complexity, they can often lead to [_overfitting_](https://www.tmwr.org/tuning.html#overfitting-bad). 

* This is where the model does very well on the training set but poorly on new data. 

Using _resampling_ to estimate performance can help identify parameters that lead to overfitting. 

The cost is computational time. 

## Overfitting with a support vector machine

```{r overfitting, echo = FALSE, out.width = '60%', fig.align='center', dev = 'svg', dev.args = list(bg = "transparent"), fig.width=9, fig.height=7.5, cache = TRUE}
library(tidymodels)
library(patchwork)
data(parabolic)

set.seed(91)
split <- initial_split(parabolic, strata = "class", prop = 1/2)

training_set <- training(split)
testing_set  <-  testing(split)

data_grid <-
  crossing(X1 = seq(-6, 5, length = 200),
           X2 = seq(-6, 5, length = 200))


two_class_rec <-
  recipe(class ~ ., data = parabolic) %>%
  step_normalize(all_numeric_predictors())

svm_mod <-
  svm_rbf(cost = tune(), rbf_sigma = 1) %>%
  set_engine("kernlab") %>%
  set_mode("classification")

svm_wflow <-
  workflow() %>%
  add_recipe(two_class_rec) %>%
  add_model(svm_mod)

vals <- c("underfit", "about right", "overfit")
svm_res <-
  tibble(
    cost = c(0.005, 0.5, 1000),
    label = factor(vals, levels = vals),
    train = NA_real_,
    test = NA_real_,
    model = vector(mode = "list", length = 3)
  )

for (i in 1:nrow(svm_res)) {
  set.seed(27)
  tmp_mod <-
    svm_wflow %>% finalize_workflow(svm_res %>% slice(i) %>% select(cost)) %>%
    fit(training_set)
  svm_res$train[i] <-
    roc_auc_vec(training_set$class,
                predict(tmp_mod, training_set, type = "prob")$.pred_Class1)
  svm_res$test[i]  <-
    roc_auc_vec(testing_set$class,
                predict(tmp_mod, testing_set, type = "prob")$.pred_Class1)
  svm_res$model[[i]] <- tmp_mod
}


te_plot <-
  svm_res %>%
  mutate(probs = map(model, ~ bind_cols(
    data_grid, predict(.x, data_grid, type = "prob")
  ))) %>%
  dplyr::select(label, probs) %>%
  unnest(cols = c(probs)) %>%
  ggplot(aes(x = X1, y = X2)) +
  geom_point(
    data = testing_set,
    aes(col = class),
    alpha = .75,
    cex = 3/4,
    show.legend = FALSE
  ) +
  geom_contour(aes(z = .pred_Class1), breaks = 0.5, col = "black") +
  facet_wrap( ~ label, nrow = 1) +
  ggtitle("Test Set") +
  labs(x = "Predictor A", y = "Predictor B") +
  theme_bw()

tr_plot <-
  svm_res %>%
  mutate(probs = map(model, ~ bind_cols(
    data_grid, predict(.x, data_grid, type = "prob")
  ))) %>%
  dplyr::select(label, probs) %>%
  unnest(cols = c(probs)) %>%
  ggplot(aes(x = X1, y = X2)) +
  geom_point(
    data = training_set,
    aes(col = class),
    alpha = .75,
    cex = 3/4,
    show.legend = FALSE
  ) +
  geom_contour(aes(z = .pred_Class1), breaks = 0.5, col = "black") +
  facet_wrap( ~ label, nrow = 1) +
  ggtitle("Training Set") +
  labs(x = "Predictor A", y = "Predictor B") +
  theme_bw()
tr_plot / te_plot
```

# Choosing tuning parameters

Let's take our previous model and add a few changes:

```{r eval = FALSE}
elevators_rec <- recipe(speed_fpm ~ ., data = elevators_train) %>% 
  step_date(approval_date, lastper_insp_date, keep_original_cols = FALSE) %>%
  step_impute_mean(all_numeric_predictors()) %>%
  step_novel(all_nominal_predictors()) %>%
  step_unknown(all_nominal_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>% 
  step_zv(all_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>% 
  step_corr(all_numeric_predictors(), threshold = 0.9)

lm_spec <- linear_reg() %>% 
  set_engine("lm")

elevators_wflow <- workflow() %>% 
  add_model(lm_spec) %>% 
  add_recipe(elevators_rec)
```

# Use regularized regression

Let's take our previous model and add a few changes:

```{r eval = FALSE}
#| code-line-numbers: "12"
elevators_rec <- recipe(speed_fpm ~ ., data = elevators_train) %>% 
  step_date(approval_date, lastper_insp_date, keep_original_cols = FALSE) %>%
  step_impute_mean(all_numeric_predictors()) %>%
  step_novel(all_nominal_predictors()) %>%
  step_unknown(all_nominal_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>% 
  step_zv(all_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>% 
  step_corr(all_numeric_predictors(), threshold = 0.9)

lm_spec <- linear_reg() %>% 
  set_engine("glmnet")

elevators_wflow <- workflow() %>% 
  add_model(lm_spec) %>% 
  add_recipe(elevators_rec)
```

# Add mode parameters

```{r eval = FALSE}
#| code-line-numbers: "11"
elevators_rec <- recipe(speed_fpm ~ ., data = elevators_train) %>% 
  step_date(approval_date, lastper_insp_date, keep_original_cols = FALSE) %>%
  step_impute_mean(all_numeric_predictors()) %>%
  step_novel(all_nominal_predictors()) %>%
  step_unknown(all_nominal_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>% 
  step_zv(all_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>% 
  step_corr(all_numeric_predictors(), threshold = 0.9)

lm_spec <- linear_reg(penalty, mixture) %>% 
  set_engine("glmnet")

elevators_wflow <- workflow() %>% 
  add_model(lm_spec) %>% 
  add_recipe(elevators_rec)
```

# Mark them for tuning

```{r eval = FALSE}
#| code-line-numbers: "11"
elevators_rec <- recipe(speed_fpm ~ ., data = elevators_train) %>% 
  step_date(approval_date, lastper_insp_date, keep_original_cols = FALSE) %>%
  step_impute_mean(all_numeric_predictors()) %>%
  step_novel(all_nominal_predictors()) %>%
  step_unknown(all_nominal_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>% 
  step_zv(all_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>% 
  step_corr(all_numeric_predictors(), threshold = 0.9)

lm_spec <- linear_reg(penalty = tune(), mixture = tune()) %>% 
  set_engine("glmnet")

elevators_wflow <- workflow() %>% 
  add_model(lm_spec) %>% 
  add_recipe(elevators_rec)
```

# Remove unneeded step

```{r eval = FALSE}
elevators_rec <- recipe(speed_fpm ~ ., data = elevators_train) %>% 
  step_date(approval_date, lastper_insp_date, keep_original_cols = FALSE) %>%
  step_impute_mean(all_numeric_predictors()) %>%
  step_novel(all_nominal_predictors()) %>%
  step_unknown(all_nominal_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>% 
  step_zv(all_predictors()) %>%
  step_normalize(all_numeric_predictors())

lm_spec <- linear_reg(penalty = tune(), mixture = tune()) %>% 
  set_engine("glmnet")

elevators_wflow <- workflow() %>% 
  add_model(lm_spec) %>% 
  add_recipe(elevators_rec)
```

## Add a spline step (just for demonstration)

```{r eval = FALSE}
#| code-line-numbers: "9"
elevators_rec <- recipe(speed_fpm ~ ., data = elevators_train) %>% 
  step_date(approval_date, lastper_insp_date, keep_original_cols = FALSE) %>%
  step_impute_mean(all_numeric_predictors()) %>%
  step_novel(all_nominal_predictors()) %>%
  step_unknown(all_nominal_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>% 
  step_zv(all_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_ns(floor_to, deg_free = tune())

lm_spec <- linear_reg(penalty = tune(), mixture = tune()) %>% 
  set_engine("glmnet")

elevators_wflow <- workflow() %>% 
  add_model(lm_spec) %>% 
  add_recipe(elevators_rec)
```

# Grid search

This is the most basic (but very effective) way for tuning models. 

tidymodels has pre-defined information on tuning parameters, such as their type, range, transformations, etc. 

A grid can be created manually or automatically. 

The `parameters()` function extracts the tuning parameters and the info. 

The `grid_*()` functions can make a grid. 

# Manual grid - get parameters

```{r}
elevators_wflow %>%
  extract_parameter_set_dials()
```

This type of object can be updated (e.g. to change the ranges, etc)

# Manual grid - create grid

This is a type of _space-filling design_. 

It tends to do much better than random grids and is (usually) more efficient than regular grids. 

```{r}
set.seed(2)
grid <- 
  elevators_wflow %>% 
  extract_parameter_set_dials() %>% 
  grid_latin_hypercube(size = 25)

grid
```

# The results

::: columns
::: {.column width="40%"}
```{r, eval=FALSE}
set.seed(2)
grid <- 
  elevators_wflow %>% 
  extract_parameter_set_dials() %>% 
  grid_latin_hypercube(size = 25)

grid %>% 
  ggplot(aes(penalty, mixture)) + 
  geom_point(cex = 4) + 
  scale_x_log10()
```

Note that `penalty` was generated in log-10 units. 
:::

::: {.column width="60%"}
```{r, echo=FALSE}
set.seed(2)
grid <- 
  elevators_wflow %>% 
  extract_parameter_set_dials() %>% 
  grid_latin_hypercube(size = 25)

grid %>% 
  ggplot(aes(penalty, mixture)) + 
  geom_point(cex = 4) + 
  scale_x_log10()
```
:::
:::

# Manual grid - create grid

We will stick to a manual grid since [glmnet]{.pkg} can calculate multiple penalty values at once

```{r}
set.seed(2)
grid <- 
  elevators_wflow %>% 
  extract_parameter_set_dials() %>% 
  grid_regular(levels = c(mixture = 10, penalty = 50))

grid
```

# Grid search

The `tune_*()` functions can be used to tune models. 

`tune_grid()` is pretty representative of their syntax (and is similar to `last_fit()`): 

```{r tuning, cache = TRUE}
ctrl <- control_grid(save_pred = TRUE)
set.seed(9)
elevators_res <- tune_grid(
  object = elevators_wflow,
  resamples = elevators_folds,
  grid = grid,
  control = ctrl
)
elevators_res
```

# Grid results

```{r}
autoplot(elevators_res)
```

# Returning results

```{r}
collect_metrics(elevators_res)
```

# Returning results

```{r}
collect_metrics(elevators_res, summarize = FALSE)
```

# Picking a parameter combination

You can create a tibble of your own or use one of the `tune::select_*()` functions: 

```{r}
show_best(elevators_res, metric = "rmse")
```

# Picking a parameter combination

We can also select a little better, by picking the most simple model whose loss of performance is within some acceptable limit.

```{r}
smallest_rmse <- select_by_pct_loss(
  elevators_res, 
  metric = "rmse",
  desc(penalty),
)
smallest_rmse
```

# Picking a parameter combination

```{r}
autoplot(elevators_res) +
  geom_vline(xintercept = smallest_rmse$penalty)
```

# Updating the workflow and final fit

```{r}
elevators_wflow <-
  elevators_wflow %>% 
  finalize_workflow(smallest_rmse)

test_res <- 
  elevators_wflow %>% 
  last_fit(split = elevators_split)
test_res
```

The workflow, fit using the training set:

```{r}
final_chi_wflow <- 
  test_res$.workflow[[1]]
```

# Test set results

```{r}
collect_metrics(test_res)

# Resampling results
show_best(elevators_res, metric = "rmse", n = 1)
```

# Plot performance

::: columns
::: {.column width="40%"}

```{r, eval=FALSE}
test_res %>% 
  collect_predictions() %>%
  ggplot(aes(.pred, speed_fpm)) + 
  geom_abline(lty = 2, col = "green") +
  geom_point(alpha = 0.3, cex = 2) +
  coord_obs_pred() +
  theme_minimal()
```
:::

::: {.column width="50%"}
```{r, echo=FALSE}
test_res %>% 
  collect_predictions() %>%
  ggplot(aes(.pred, speed_fpm)) + 
  geom_abline(lty = 2, col = "green") +
  geom_point(alpha = 0.3, cex = 2) +
  coord_obs_pred() +
  theme_minimal()
```
:::
:::

# Hands-On: Tune hyperparameters

Go to the lab and finish the document by tuning a model
